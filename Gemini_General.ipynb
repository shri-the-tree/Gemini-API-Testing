{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: General Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers:\n",
        "\n",
        "- Temperature\n",
        "- Max output length\n",
        "- Token counting\n"
      ],
      "metadata": {
        "id": "QOKxmDLtRogx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTEz0qtFvxC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAK8IsGF4Os"
      },
      "source": [
        "Since we've put our gemini API key in Colab Secrets, we can just run the following cells to setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd36fa68-7a13-44c4-d971-11017e64511d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Model Temperature\n",
        "\n",
        "Steve is indecisive about how to spend his Friday night as a Freshman at Berkeley. He decides to ask Gemini using the 1.5 Flash [variant](https://ai.google.dev/gemini-api/docs/models/gemini):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "2bcfnGEviwTI",
        "outputId": "35ee31a4-94e1-4db4-8be0-e461713688c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Host a \"Tech Talks and Trivia\" night with a focus on ethical AI and the future of work, offering pizza and board games for a relaxed atmosphere where you can connect with like-minded peers. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley looking for emotionally mature and ambitious friends in a single one-sentence idea\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwOhbdVkD_c"
      },
      "source": [
        "Steve is very smart and knows that Gemini is non-deterministic; the same prompt can result in different outputs! To demonstrate this, the following code passes the same prompt 5 different times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oxc8f6oQlCAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "09795b17-c9f3-420b-ecf4-63b229c374bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Host a low-key \"coding for good\" hackathon at your apartment, focusing on projects that address social issues, inviting fellow ambitious CS majors for a night of tech and meaningful discussion. \n",
            "\n",
            "2. Host a board game night with a \"hackathon-inspired\" twist, inviting fellow CS majors to compete for bragging rights and maybe even a prize. \n",
            "\n",
            "3. Host a casual board game night at a park with a focus on strategy games, inviting fellow CS majors to share their competitive spirit and discuss their ambitious goals. \n",
            "\n",
            "4. Host a board game night at your apartment, focusing on strategy games like Settlers of Catan or Ticket to Ride, to attract fellow ambitious CS majors seeking intellectually stimulating company. \n",
            "\n",
            "5. Host a \"Hackathon for Humanity\" themed game night with coding puzzles and discussions about impactful tech projects, attracting like-minded friends with a passion for making a difference. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "outputs = []\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley looking for emotionally mature and ambitious friends in a single one-sentence idea\"\n",
        "for i in range(5):\n",
        "  response = model.generate_content(prompt)\n",
        "  outputs.append(response.text)\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA9ewE_dMFMZ"
      },
      "source": [
        "Gemini returns a different response each time. Hmm. Steve doesn't like the fact that his Friday night might be determined by random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFTcYStQ2YY"
      },
      "source": [
        "Fortunately, Gemini has a temperature parameter that controls the randomness of the output. Temperature values can range from 0.0 to 2.0. We can check the temperature of our current model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GPndrGi2nP5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "4c1a2e31-7db2-4857-fc67-0d50477a6434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == 'models/gemini-1.5-flash':\n",
        "        print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdqPso3kamW"
      },
      "source": [
        "Let's initialize a new model with a temperature of 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-GqupV-dYsvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d69fc054-8b2d-4526-cdf4-db6d92c82e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Host a \"Code & Cocktails\" night at a trendy bar, inviting fellow CS majors to brainstorm innovative projects while enjoying drinks and conversation. \n",
            "\n",
            "2. Host a \"Coding for Change\" hackathon at a local non-profit, inviting fellow CS majors to build tech solutions for social good while connecting with like-minded individuals. \n",
            "\n",
            "3. How about attending a hackathon focused on social good and using your coding skills to make a positive impact while connecting with like-minded individuals? \n",
            "\n",
            "4. Host a board game night with a focus on strategy games, inviting fellow CS majors for some competitive fun and intellectually stimulating conversation. \n",
            "\n",
            "5. Host a board game night with a focus on strategy games and lively discussion, inviting fellow CS majors to connect and build friendships. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "new_outputs = []\n",
        "low_temp_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.798})\n",
        "for i in range(5):\n",
        "  response = low_temp_model.generate_content(prompt)\n",
        "  new_outputs.append(response.text)\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs are the same! Finally Steve can be sure how to spend his night. Conversely, setting temperature to the max value of 2.0 would have the opposite effect, yielding more unpredictable responses."
      ],
      "metadata": {
        "id": "AZ78-JuN7XHi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5QDIG6cPbID"
      },
      "source": [
        "## Max Output Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MfrBmbPi5i"
      },
      "source": [
        "Let's say Steve wants his outputs to be below a certain length. In large language models, text is generated in tokens. For Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words. He can set the `max_output_tokens` variable as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69bfb7a6-11ae-46f3-a1f3-66b4eecaca37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grab some friends, code up a fun project, and watch the sunset from the top of the\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "short_response_model = genai.GenerativeModel(model_name, generation_config={\"max_output_tokens\": 19})\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = short_response_model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz31PRYSRMUt"
      },
      "source": [
        "Notice that this simply halts token generation at a fixed quantity and does not guarantee that the output is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4672af98ac57"
      },
      "source": [
        "## Token Count\n",
        "\n",
        "Let's say that Steve is being charged for every token that he inputs to Gemini.\n",
        "\n",
        "If Steve has billing enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important. As such, Steve might want to know the number of tokens in his prompt (input) before actually putting it into the model.\n",
        "\n",
        "Let's create a new instance of Gemini 1.5 Flash and set our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "338fb9a6af78"
      },
      "outputs": [],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "token_n_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.0})\n",
        "poem_prompt = \"Write me a poem about Berkeley's data science wing of the campus\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = token_n_model.generate_content(poem_prompt)\n",
        "response.text"
      ],
      "metadata": {
        "id": "9WVlpLcDLjOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "bb213816-2d2f-4b14-f348-404017be14db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"On Berkeley's hills, where knowledge thrives,\\nA new domain, where data strives.\\nA wing of science, bold and bright,\\nWhere numbers dance in digital light.\\n\\nFrom algorithms, complex and deep,\\nTo insights hidden, secrets to keep.\\nThe data scientists, minds so keen,\\nUnraveling patterns, unseen, unseen.\\n\\nWith Python code and R's command,\\nThey build models, understand,\\nThe flow of information, vast and wide,\\nUnveiling truths, where secrets hide.\\n\\nFrom social networks, to markets' sway,\\nThey analyze trends, day by day.\\nPredicting futures, with precision's art,\\nA symphony of data, playing its part.\\n\\nIn classrooms bright, and labs so grand,\\nThey learn and teach, hand in hand.\\nThe future's promise, in their grasp,\\nA data-driven world, a future to clasp.\\n\\nSo raise a glass, to Berkeley's might,\\nWhere data science shines, so clear and bright.\\nA beacon of knowledge, for all to see,\\nThe future's promise, eternally. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating a response, we can check how many tokens are in this prompt using the `.count_tokens` function of the model:"
      ],
      "metadata": {
        "id": "-1w3HL5nIyR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_count = token_n_model.count_tokens(poem_prompt)\n",
        "output_token_count = token_n_model.count_tokens(response.text)\n",
        "print(f'Tokens in prompt: {prompt_token_count} \\n Estimated tokens in output {output_token_count}')"
      ],
      "metadata": {
        "id": "lJ0V8H2dIf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a58c55dd-7436-47fa-9b75-5674b2c2f478"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in prompt: total_tokens: 14\n",
            " \n",
            " Estimated tokens in output total_tokens: 238\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}